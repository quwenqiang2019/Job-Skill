# 1、大模型开发-----数据

- 预训练数据（语料数据）
	- 预训练语料的来源可以大致分为两类：
		- 通用文本数据General Text Data
			- 网页Webpages
				- 为了方便使用这些数据资源，在以前的工作中，大量的数据被从网上抓取，如CommonCrawl。
			- 对话文本Conversation text
				- 研究人员可以利用公共会话语料库的子集（如PushShift.io Reddit语料库）或从在线社交媒体收集会话数据。
			- 书籍Books
				- 为了获得开源的书籍数据，现有的研究通常采用Books3和Bookcorpus2数据集，这些数据集可以在Pile数据集中找到。
		- 专门的文本数据Specialized Text Data
			- 多语言文本Multilingual text
				- BLOOM和PaLM在其预训练语料库中分别策划了涵盖46种和122种语言的多语言数据
			- 科学文本Scientific text
				- 收集arXiv论文、科学教科书、数学网页和其他相关科学资源。
				- 由于科学领域数据的复杂性，如数学符号和蛋白质序列，通常**需要特定的标记化和预处理技术来将这些不同格式的数据转化为可由语言模型处理的统一形式**。
			- **代码Code**
				- 第一种来源是来自编程问题回答社区，如Stack Exchange。
				- 第二个来源是公共软件库，如GitHub，在那里收集代码数据（包括评论和文档字符串）以供利用。


- 指令微调数据
	- instruction: 指令
	- input: 输入（可以为空）
	- output: 输出

- 人类反馈强化数据


- 评测数据


# 2、大模型的训练流程
- 假设我们选择ChatGLM为基座模型，想经过训练、微调、优化得到领域适配模型CodeDev

![](pic/38.jpg)

- 整体上可以分为三个阶段，分别如下：
	- 预训练（Pre-Training,PT），基于ChatGLM-6B的初始模型，经过海量代码语料训练，得到领域适配的ChatGLM-6B
	- 监督微调(Supervised Finetuning, SFT)--------指令微调，通过基于xxxx等数据，构建训练数据完成指令微调
	- 对齐微调（Alignment Tuning）----------基于人类反馈的强化学习（RLHF）：模型通过与环境互动和接收反馈来学习。对模型的训练是为了最大化奖励信号（使用 PPO），奖励信号通常来自人类对模型输出的评估。


- 第一阶段：PT
	- 预训练是LLM训练的第一阶段，它在**大规模未标记的文本数据集上进行**。这个阶段的主要目标是使模型吸收大量的知识和语言结构。预训练阶段采用了自监督学习方法，其中模型预测给定上下文下一个词或令牌是什么。
		- 数据集选择：通常使用包含数十亿到数万亿令牌的庞大文本语料库，如互联网文本。
		- 自监督学习：模型根据文本数据的上下文预测下一个词，这种任务称为下一个词预测。通过这个任务，模型学习了语法、语义和常识知识。
- 第二阶段：SFT
	- 监督微调是LLM训练的第二阶段，其目的是根据特定任务或指令微调模型，使其更适合执行具体的自然语言处理任务。与预训练不同，监督微调需要**特定的标记数据集**，其中包含了指令和相应的期望输出。
		- 数据集**准备**：人类编写指令-输出对，指令是模型的输入，期望输出是我们期望模型生成的内容。这通常需要耗费大量人力和时间。训练数据格式：整体的结构采用instruction/input/output，数据集可以以文本文件（如CSV、JSON等）或数据库的形式存储。每个样本包含指令和对应的期望输出。可以使用表格形式存储数据，每一列代表一个指令或输出。
		- 任务多样性：监督微调可以应用于各种任务，如翻译、问题回答、摘要生成等。
		- 微调策略：模型参数根据监督数据的误差进行微调，以提高在指定任务上的性能。
- 对齐（Alignment）：基于人类反馈的强化学习（RLHF）
	- 目标是将LLM的输出与人类偏好对齐，以进一步提高模型的性能、帮助性和安全性。这一阶段常常涉及强化学习与人类反馈（RLHF）的应用。
	- RLHF是对齐阶段的核心。它为模型提供了一个持续学习和改进的机制。在RLHF中，模型生成文本，并接收人类提供的反馈信息。
		- 反馈回路：RLHF创建了一个循环，模型生成文本，然后根据人类反馈进行调整。反馈可以是积极的，表示模型生成的文本是合适的，也可以是消极的，表示需要改进。
		- 渐进改进：通过多次迭代，模型逐渐提高其生成文本的质量，以更好地满足用户的需求。这种渐进改进有助于模型更好地理解人类的期望和意图。
		- 安全性提升：RLHF还可以用于提高模型的安全性，减少不当内容的生成。模型可以通过学习避免不恰当的回应来提高安全性。
		- 训练数据格式：整体的结构采用Input/Reward
		- 数据集格式：数据集可以以文本文件（如CSV、JSON等）或数据库的形式存储。每个样本包含输入数据和对应的奖励数据。可以使用表格形式存储数据，每一列代表一个特征或标签。

# 3、大模型微调
- 本质上，现在的大模型要解决的问题，就是一个序列数据转换的问题：
	- 输入序列 X = [x1, x2, ..., xm]， 输出序列Y = [y1, y2, …, yn]，X和Y之间的关系是：Y = WX。
- 从参数规模的角度，大模型的微调分成两条技术路线：
	- 一条是对全量的参数，进行全量的训练，这条路径叫全量微调FFT(Full Fine Tuning)。
		- FFT的原理，就是用特定的数据，对大模型进行训练，将`W`变成`W‘`，`W``相比`W`，最大的优点就是上述特定数据领域的表现会好很多。
	- 一条是只对部分的参数进行训练，这条路径叫PEFT(Parameter-Efficient Fine Tuning)。
		- Prompt Tuning
			- Prompt Tuning是在Embedding环节，往输入序列X前面加特定的Token。
			- 就是将`X = [x1, x2, ..., xm]`变成，X' = [x'1, x'2, ..., x'k; x1, x2, ..., xm], Y = WX'。
		- Prefix Tuning
			- Prefix Tuning是在Transformer的Encoder和Decoder的网络中都加了一些特定的前缀。
			- 就是将Y=WX中的W，变成W'= [Wp; W]，Y=W'X。
		- LoRA
			- 大模型参数很多，但并不是所有的参数都是发挥同样作用的
		- QLoRA
			- QLoRA就是量化版的LoRA，它是在LoRA的基础上，进行了进一步的量化，将原本用16bit表示的参数，降为用4bit来表示，可以在保证模型效果的同时，极大地降低成本。