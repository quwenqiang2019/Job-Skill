

##1、自然语言处理有六个大的任务
- LLM 使用**深度神经网络（例如 transformers）**从数十亿或数万亿个单词中学习，并生成关于任何主题或领域的文本，还可以执行各种自然语言任务：
	- 分类：从文字序列到标签的映射，如文本分类。
	- 匹配：文字序列与文字序列的匹配，如搜索、阅读理解。
	- 标注和语义分析：文字序列到标签序列或结构表示的映射，如分词、词性标注、句法分析。
	- 序列生成：文字序列的生成，也就是基于语言模型的生成。
	- 序列到序列（seq2seq）：文字序列到文字序列的转化，如机器翻译、生成式对话、摘要。
	- 序贯决策：基于已有的文字序列产生新的文字序列，如多轮对话。
	- 前三个是语言理解任务，后三个是语言生成任务。


## 2、语言模型与大语言模型
- 语言模型（Language Model）是一种机器学习算法，它可以根据给定文本来预测下一个词语或字符的出现的概率。语言模型通过大量的文本数据来学习语言的统计特征，进而生成具有相似统计特征的新文本。其核心目标是建立一个**统计模型**，用来估计文本序列中每个词语或字符出现的概率，从而实现语言生成、语言理解等自然语言处理任务。
- 大模型主要指具有数十亿甚至上百亿参数的深度学习模型，其具备较大容量、海量参数、大算力等特点。大模型由早期的单语言预训练模型发展至多语言预训练模型，再到现阶段的多模态预训练模型，可实现处理多任务的目标。 
	- 大语言模型与普通语言模型相比，大语言模型的一个显著区别在于其规模。大语言模型通常具有**大量的参数**，并且在**训练过程中使用了巨量的文本数据**。
	- 为帮助语言模型更好地理解每个词的特征和含义， 我们需要使用大量的参数来存储和处理信息。我们会将这些**词嵌入**一个**高维的向量空间**里面，像GPT-3的向量空间的维数就有12288，这意味着GPT-3可以使用12288个维度来充分理解某个词。对于很多单词的理解可能比人类都要透彻。

## 三种大模型架构
 ![](./pic/p24.jpg)


## 3、大语言模型的开发技术
- 预训练：预训练是在针对特定任务进行微调之前，在大型数据集（通常是无监督或自监督）上训练 LLM 的过程。在预训练期间，模型学习一般语言模式、单词之间的关系以及其他基础知识。此过程产生预训练模型，可以使用较小的特定于任务的数据集进行微调，从而显着减少在各种 NLP 任务上实现高性能所需的标记数据量和训练时间。
	- 数据收集：在预训练阶段，会收集来自互联网的大规模文本语料库。通常，这个语料库包含来自各种来源的多样化文本。
	- 分词：收集的文本数据被分割成称为标记的较小单元，可以是单词或子词单位。
	- 模型架构：为预训练选择了一个神经网络架构，通常基于变压器模型。
	- 训练目标：模型被训练以预测基于前面标记的概率分布中的下一个标记。这被称为语言建模目标。
	- 大规模训练：预训练需要大量计算资源，通常在强大的硬件上进行，使用大批量大小和多个GPU或TPU。
	- 检查点：在训练过程中定期保存模型参数的快照（检查点）。
- 迁移学习（微调）：迁移学习是一种利用预训练期间获得的知识并将其应用于新的相关任务的技术。在LLM的背景下，迁移学习涉及在较小的特定任务数据集上微调预训练模型，以实现该任务的高性能。迁移学习的好处在于，它允许模型从预训练期间学到的大量通用语言知识中受益，从而减少对大型标记数据集和每个新任务的大量训练的需求。
	- 任务特定数据集：对于微调，需要准备一个与特定自然语言处理（NLP）任务相关的数据集。这个数据集包含了用于监督任务的标记示例，或者对于强化学习任务来说是奖励信号。
	- 迁移学习：加载预训练的基础模型，该模型已经学会了一般的语言理解。微调涉及根据任务特定数据集来更新模型的参数。
	- 微调策略：使用各种技术，如梯度下降，来更新模型的参数，同时最小化任务特定的损失函数。
	- 超参数调整：微调可能涉及调整超参数，如学习率、批量大小和丢弃率，以优化任务性能。
	- 评估：在验证集上评估微调后的模型性能，并根据需要进行调整。
	- 部署：一旦微调后的模型在验证集上表现良好，就可以将其部署到特定任务的实际应用中。


## 4、基于 Transformer 的 LLM 模型架构
大语言模型在很大程度上代表了一类称为Transformer网络的深度学习架构。Transformer模型是一个神经网络，通过跟踪序列数据中的关系（像这句话中的词语）来学习上下文和含义。


- 输入模块
	- 标记化（Tokenization）：标记化是将文本序列转换为模型可以理解的**单个单词、子词或标记**的过程。在LLM中，标记化通常使用字节对编码 (BPE) 或 WordPiece 等子字算法来执行，这些算法将文本分割成更小的单元，以捕获频繁和罕见的单词。这种方法有助于限制模型的词汇量大小，同时保持其表示任何文本序列的能力。
	- 词嵌入（Embbeding）：输入文本被**标记**为更小的单元，例如单词或子词，并且每个标记被**嵌入**到连续向量表示中。此嵌入步骤捕获输入的语义和句法信息。嵌入是单词或标记的连续向量表示，可捕获它们在高维空间中的语义。它们允许模型将离散标记转换为神经网络可以处理的格式。在LLM中，嵌入是在训练过程中学习的，所得的向量表示可以捕获单词之间的复杂关系，例如同义词或类比。
	- 位置编码：位置编码被添加到输入嵌入中以提供有关标记位置的信息，因为转换器不会自然地对标记的顺序进行编码。这使得模型能够处理标记，同时考虑它们的顺序。

- 编码器模块（给定文本的输入提取文本的特征，将输入序列转化为上下文感知的表示）：基于神经网络技术，编码器分析输入文本并创建许多隐藏状态来保护文本数据的上下文和含义。多个编码器层构成了 Transformer 架构的核心。
	- 自注意力机制：自注意力使模型能够通过计算**注意力分数**来权衡输入序列中不同标记的重要性。它允许模型以上下文感知的方式考虑不同标记之间的依赖关系和关系。LLM 中的注意力机制，特别是 Transformer 中使用的自注意力机制，允许模型权衡给定上下文中不同单词或短语的重要性。通过为输入序列中的标记分配不同的权重，模型可以专注于最相关的信息，同时忽略不太重要的细节。这种有选择地关注输入的特定部分的能力对于捕获远程依赖性和理解自然语言的细微差别至关重要。
		- 多头注意力：Transformers 通常采用多头注意力，其中自注意力与不同的学习注意力权重同时执行。这使得模型能够捕获不同类型的关系并同时处理输入序列的各个部分。可以捕获单词之间多种维度上的相关系数 attention score。
	- 残差连接：用于解决多层网络训练的问题，可以让网络只关注当前差异的部分
	- 前馈神经网络：在自注意力步骤之后，前馈神经网络独立地应用于每个标记。该网络包括具有非线性激活函数的完全连接层，使模型能够捕获令牌之间的复杂交互。
	- 规范化层：

- 解码器模块（根据编码器的输出和目标序列生成一个新的序列）
	- 解码器层：在一些基于变压器的模型中，除了编码器之外还包含解码器组件。解码器层支持自回归生成，其中模型可以通过关注先前生成的标记来生成顺序输出。


- 输出模块
	- 层归一化：层归一化应用于变压器架构中的每个子组件或层之后。它有助于稳定学习过程并提高模型泛化不同输入的能力。
	- 输出层：变压器模型的输出层可以根据具体任务而变化。例如，在语言建模中，通常使用线性投影和 SoftMax 激活来生成下一个标记的概率分布。
- 最重要的是要记住，基于 Transformer 的模型的实际架构可以根据特定的研究和模型创建进行更改和增强。为了完成不同的任务和目标，GPT、BERT 和 T5 等多种模型可能会集成更多组件。

 ![](./pic/p23.jpg)

- input：是一个词，比如：机器
- input--->embedding：是一个词向量，比如：x1=[0.2,0.3,0.8,0.5]
- 词向量+位置编码输入到多头注意力层中：比如加上位置编码[0,0,1,1]，x1=[0.2, 0.3, 1.8, 1.5]，经过self-attention得到得到多个不同的权重矩阵Z，将多个Z进行拼接就得到了self-attention层的输出，把这些矩阵拼接起来然后用一个额外的权重矩阵与之相乘，把这8个矩阵压缩成一个z1，最终的z1就作为前馈神经网络的输入。z1经过前馈神经网络输出r1
- r1进入下一个编码器
- 

## 5、大模型的未来的两大趋势
- bigger and smarter
	- 主流的大模型会越做越大，越做越聪明，基座大模型出现跟移动操作系统类似的格局
		- ios（chatgpt）
		- 安卓（LLaMA）
		- 鸿蒙（国产的文心一言或其他）
- free and smaller
	- 大模型会越做越小，放到手机上，学习你手机的数据，形成你的一个AI分身


## 6、如何在业务中应用大语言模型
- 确定需求：辅助软件自动化智能化构建
- 选择合适的型号（基座模型）： OpenAI 的 GPT、Google 的 BERT以及基于 Transformer 的模型
- 访问模型：开源/闭源
- 数据预处理：消除不相关的信息、纠正错误以及将数据转换为大语言模型可以轻松理解的格式
- 微调模型：针对专门用例优化模型参数
- 实施模型：将大语言模型嵌入到反馈系统中
- 监控和更新模型：随着需求的变化，调整模型的参数


## 几个相关的术语
- 提示（Prompts）
	- 提示是交流和指导 LLM AI 模型行为的方式。提示是用户或程序向 LLM AI 提供的输入或查询，以便从模型中引出特定的响应。提示可以是自然语言句子或问题，也可以是代码片段或命令，也可以是文本或代码的任意组合，具体取决于领域和任务。
- 令牌（Tokens）
	- 令牌是 LLM AI 用来处理和生成语言的文本或代码的基本单位，可以是字符、单词、子词或其他文本或代码片段，具体取决于所选的令牌化方法或方案。它被分配数值或标识符，并按序列或向量排列，并被输入模型或从模型输出。令牌是模型语言的构建块。
- 嵌入（Embeddings）
	- 嵌入是令牌在高维向量空间中的表示或编码，其中每个维度对应于学习到的语言特征或属性。嵌入是模型捕获和存储语言的含义和关系的方式，也是模型比较和对比不同标记或语言单元的方式。嵌入是模型语言的离散和连续、符号和数字方面之间的桥梁。
- 向量数据库（Vector Database）
	- 向量数据库是一种将数据存储为高维向量的数据库，高维向量是特征或属性的数学表示。每个向量都有一定数量的维度，范围从几十到几千不等，具体取决于数据的复杂性和粒度。向量通常是通过对原始数据（例如：文本、图像、音频、视频等）应用某种变换或嵌入函数来生成的。嵌入函数可以基于各种方法，例如：机器学习模型、词嵌入、特征提取算法。

## 基座大模型

![](./pic/p22.png)



##大模型训练优化
- 大模型训练优化需要解决算法、数据、算力三个方向的问题
	- 算法（Algorithm）：算法是一系列解决问题的步骤和规则。算法的设计和优化可以提高计算效率，使得相同的计算任务能够在更短的时间内完成，或者使用更少的资源。
	- 数据（Data）：数据是算法的输入和输出。数据的质量和规模对算法的性能和准确性有重要影响。好的数据能够提供更准确的结果，并帮助改进算法的训练和优化过程。
	- 算力（Computational Power）：算力指的是计算机系统的计算能力和资源。算力的提升可以支持更复杂的算法和处理更大规模的数据。高算力的计算机系统可以加速算法的执行，提高应用程序的性能和响应速度。


##大模型相关的技术
- 提示工程（prompt engineering）
- Agent开源框架
- LangChain框架
- 分布式训练并行策略
- 指令微调（Instruction Tuning）
- Supervised Fine-Tuning（ＳＦＴ）
- 参数高效的微调技术（Parameter-Efficient Fine-Tuning）
- 基于人类反馈的强化学习（Reinforcement Learning from Human Feedback，RLHF）
- Retrieval Augmented Generation (RAG， 检索增强生成)



