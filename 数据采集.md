## 数据采集（爬虫）-使用Request库获取到HTML网页
- response = requests.get(url,params,headers)
	- url: 必需参数，表示要请求的URL地址。
	- params: 可选参数，用于指定请求的查询参数。可以是一个字典、列表或字符串。
	- headers: 可选参数，用于指定请求头部信息。可以是一个字典。
	- cookies: 可选参数，用于指定请求的Cookies。可以是一个字典。
	- timeout: 可选参数，用于指定请求的超时时间。可以是一个浮点数或元组。
	- proxies: 可选参数，用于指定请求的代理服务器。可以是一个字典。
	- verify: 可选参数，用于指定SSL证书验证方式。可以是一个布尔值或字符串。
	- stream: 可选参数，表示是否使用流式传输。可以是一个布尔值。
- requests.post()
	- requests.post 方法用于发送 HTTP POST 请求，它会向指定的 URL 发送请求，并将请求数据作为请求体发送给服务器。POST 请求通常用于提交表单数据、上传文件等操作。




## access_token和cookie的区别
- access_token 和 cookie 都是用于身份验证的机制，但它们在使用方式和作用范围上有所不同。
- Access_token 是一种令牌，通常用于 API 调用时进行身份验证。在用户登录后，服务器会生成一个 access_token 并返回给客户端，客户端在接下来的 API 调用中需要携带该 access_token，以便服务器能够识别和验证该用户的身份。access_token 的作用范围通常只限于 API 调用，不会涉及到整个网站或应用程序。
- Cookie 是一种存储在客户端浏览器中的数据，通常用于网站的身份验证和状态维护。在用户登录后，服务器会生成一个包含用户信息的 cookie 并返回给客户端，客户端在接下来的请求中需要携带该 cookie，以便服务器能够识别和验证该用户的身份。cookie 的作用范围通常涵盖整个网站或应用程序，可以用于维护用户的登录状态、存储用户的偏好设置等。
- 无API情况（Cookie）
	- 无接口，需要借助第三方库对html进行解析（分为面向单个网页和面向多个网页），提取出所需的相关信息。
	- Info = response.text
	- soup = BeautifulSoup(response.text, 'html.parser')
- 有API情况（Access_token）
	- 有接口，人家已经把相关的信息采集好了以json格式进行了组织，不需要再去解析html，直接从json读取所需信息即可。例如：利用GitHub和Gitee 的API获取PR相关信息。
	- Info = response.json()


## 使用BeautifulSoup框架解析HTML文档并提取所需内容
理解html文档的基本格式

```
<html>
  <head>
    <title>Example Page</title>
  </head>
  <body>
    <h1>Example Page</h1>
    <p>This is an example page with some <a href="http://www.google.com">links</a>.</p>
    <p>Here is another <a href="http://www.yahoo.com">link</a>.</p>
  </body>
</html>
```

- <body>标记用于定义HTML文档所要显示的内容，也称为主体标记。浏览器中显示的所有文本、图像、音频和视频等信息都必须位于<body>标记内，<body>标记中的信息才是最终展示给用户看的。一个HTML文档只能有一对<body>标记，且<body>标记必须在<html>标记内，位于<head>头部标记之后，与<head>标记是并列关系。

- 在此标签中可以包含
`<p>（段落标签）`
`<h1>（标题标签）`
`<br>（换行标签）`
`<div>（区块/容器标签）`等众多标签
- <body>标签中还有很多属性，用于设置文档的背景颜色、文本颜色、链接颜色、边距等
- html标签与标签之间的关系分别为父子关系（嵌套关系）和兄弟关系（并列关系）
- HTML网页的内容分为：
	- 静态生成的内容，直接从HTML源码中就能找到看到的数据和内容。
	- 动态生成的内容，能够在浏览器上看到，但是在HTML源码中却发现不了。


## python 如何爬取JavaScript动态生成的标签（JS动态内容）
- 不需要点击，从网页响应中找到JS脚本返回的JSON数据；
- 需要点击才能显示的信息，使用Selenium对网页进行模拟访问 

## Python中使用request库设置socks5代理
```
proxies = {
        'http': 'socks5://127.0.0.1:21881',
        'https': 'socks5://127.0.0.1:21881'
}
response = requests.get(url, proxies=proxies)
```