## 数据采集（爬虫）-使用Request库获取到HTML网页
- response = requests.get(url,params,headers)
	- url: 必需参数，表示要请求的URL地址。
	- params: 可选参数，用于指定请求的查询参数。可以是一个字典、列表或字符串。
	- headers: 可选参数，用于指定请求头部信息。可以是一个字典。
	- cookies: 可选参数，用于指定请求的Cookies。可以是一个字典。
	- timeout: 可选参数，用于指定请求的超时时间。可以是一个浮点数或元组。
	- proxies: 可选参数，用于指定请求的代理服务器。可以是一个字典。
	- verify: 可选参数，用于指定SSL证书验证方式。可以是一个布尔值或字符串。
	- stream: 可选参数，表示是否使用流式传输。可以是一个布尔值。
- requests.post()
	- requests.post 方法用于发送 HTTP POST 请求，它会向指定的 URL 发送请求，并将请求数据作为请求体发送给服务器。**POST 请求通常用于提交表单数据、上传文件等操作**。


## access_token和cookie的区别
- access_token 和 cookie 都是用于身份验证的机制，但它们在使用方式和作用范围上有所不同。
- Access_token 是一种令牌，通常用于 API 调用时进行身份验证。在用户登录后，服务器会生成一个 access_token 并返回给客户端，客户端在接下来的 API 调用中需要携带该 access_token，以便服务器能够识别和验证该用户的身份。access_token 的作用范围通常只限于 API 调用，不会涉及到整个网站或应用程序。
- Cookie 是一种存储在客户端浏览器中的数据，通常用于网站的身份验证和状态维护。在用户登录后，服务器会生成一个包含用户信息的 cookie 并返回给客户端，客户端在接下来的请求中需要携带该 cookie，以便服务器能够识别和验证该用户的身份。cookie 的作用范围通常涵盖整个网站或应用程序，可以用于维护用户的登录状态、存储用户的偏好设置等。
- 无API情况（Cookie）
	- 无接口，需要借助第三方库对html进行解析（分为面向单个网页和面向多个网页），提取出所需的相关信息。
	- Info = response.text
	- soup = BeautifulSoup(response.text, 'html.parser')
- 有API情况（Access_token）
	- 有接口，人家已经把相关的信息采集好了以json格式进行了组织，不需要再去解析html，直接从json读取所需信息即可。例如：利用GitHub和Gitee 的API获取PR相关信息。
	- Info = response.json()


## 使用BeautifulSoup框架解析HTML文档并提取所需内容
### 理解html文档的基本格式

```
<html>
  <head>
    <title>Example Page</title>
  </head>
  <body>
    <h1>Example Page</h1>
    <p>This is an example page with some <a href="http://www.google.com">links</a>.</p>
    <p>Here is another <a href="http://www.yahoo.com">link</a>.</p>
  </body>
</html>
```

- `<body>`标记
	- `<body>`标记用于定义HTML文档所要显示的内容，也称为**主体标记**。浏览器中显示的所有文本、图像、音频和视频等信息都必须位于<body>标记内，<body>标记中的信息才是最终展示给用户看的。一个HTML文档**只能有一对<body>标记**，且<body>标记必须在<html>标记内，位于<head>头部标记之后，**与<head>标记是并列关系**。
	- 在<body>标记中可以包含：
	- `<p>（段落标记）`
	- `<h1>（标题标记）`
	- `<br>（换行标记）`
	- `<div>（区块/容器标记）`
	- `<body>`标记中还有很多属性，用于设置文档的背景颜色、文本颜色、链接颜色、边距等
- html标记与标记之间的关系分别为**父子关系（嵌套关系）**和**兄弟关系（并列关系）**
- HTML网页的内容分为：
	- 静态生成的内容，直接从HTML源码中就能找到看到的数据和内容。
	- 动态生成的内容，能够在浏览器上看到，但是在HTML源码中却发现不了。


### beautifulsoup用法
- beautifulsoup是一个解析器，可以特定的解析出内容，省去了我们编写正则表达式的麻烦。
- 构建beautifulsoup实例
- soup = BeautifulSoup(html,'lxml')
	- 第一个参数是要匹配的内容
	- 第二个参数是beautifulsoup要采用的模块，即规则
- soup.find_all(name, attrs, recursive, text, limit, **kwargs)用法
- soup.find_all().find(name, attrs, recursive, text, limit, **kwargs)用法
	- name:查找标签
	- text:查找文本
	- attrs:基于attrs参数
		- id
		- class_
	- find_all()返回所有匹配到的结果，find只返回查找到的第一个结果
```

def parse_search_results(html):
    soup = BeautifulSoup(html, "html.parser")
    results = []
    search_results = soup.find_all("div", class_="result")

    for result in search_results:
        title = result.find("h3").get_text()
        link = result.find("a")["href"]
        if result.find("div", class_="content_right_8Zs40"):
            info = result.find("div", class_="content_right_8Zs40").get_text()
        else:
            info = None
        if result.find("span", class_="c-color-gray"):
            publish_info = result.find("span", class_="c-color-gray").get_text()
        else:
            publish_info = None
        if result.find("span", class_="c-color-gray2"):
            publish_time = result.find("span", class_="c-color-gray2").get_text()
        else:
            publish_time = None

        results.append({
            "title": title,
            "link": link,
            "info": info,
            "publish_info": publish_info,
            "publish_time": publish_time
        })

    return results

```

## python 如何爬取JavaScript动态生成的内容（JS动态内容）
- 不需要点击，从网页响应中找到JS脚本返回的JSON数据；
	- 即使网页内容是由JS动态生成加载的，JS也需要对某个接口进行调用，并根据接口返回的JSON数据再进行加载和渲染。所以我们可以找到JS调用的数据接口，从数据接口中找到网页中最后呈现的数据。
- 需要点击才能显示的信息，使用Selenium对网页进行模拟访问 

## Python中使用request库设置socks5代理
```
proxies = {
        'http': 'socks5://127.0.0.1:21881',
        'https': 'socks5://127.0.0.1:21881'
}
response = requests.get(url, proxies=proxies)
```

## 反爬的常见处理方案（比如返回安全验证）
- url的https换成http
- 增加请求头部信息headers一些参数
	- Cookie
	- User-Agent
	- Accept
		- ```
		headers = {
	        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
	        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
	        "Cookie": "BAIDUID=3AC36EEFD69850C8B6CDAEA1184D8DBA:FG=1; BIDUPSID=3AC36EEFD69850C8B6CDAEA1184D8DBA; PSTM=1699608130; BD_UPN=12314753; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; delPer=0; BD_CK_SAM=1; PSINO=5; BAIDUID_BFESS=3AC36EEFD69850C8B6CDAEA1184D8DBA:FG=1; BA_HECTOR=8lal8l21ah24al05850g2l0m1il61kt1q; ZFY=TZ3qFPzZe19k0LMXSJGh9OdpgO5b7FGfxPE7tLywUS0:C; H_PS_PSSID=39635_39648; H_PS_645EC=0471fZjvkAnQQ6J0r478y2rhADJWgqTfeCD4JftHEPI8CPsmQgkVHUehZGA; BDSVRTM=157; ispeed_lsm=2; WWW_ST=1699941203245",
	    }
		```
- 换ip
- 用Selenium
	- Selenium是一个用于自动化浏览器操作的工具，常用于测试网页应用程序和执行Web任务
	- 它提供了多种编程语言的客户端库，如Python、Java、C#等，用于控制浏览器的行为
	- 通过编写代码，可以模拟用户在浏览器中的操作，比如点击链接、填写表单、提交数据等，还有爬虫

- 网页内容分成两类
	- HTML静态生成的内容：直接从HTML源码中就能找到看到的数据和内容
	- 前端JS动态生成的内容：HTML源码中找不到，由JS动态生成加载，比如新闻网站的新闻数据往往都是动态的，随时会变