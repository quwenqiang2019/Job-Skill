# 1、目标和任务
- 开发出适用于软件工厂场景的大模型
- 基于大模型，开发多智能体软件开发框架（类似MetaGPT和ChatDev），通过这个框架能够实现软件的自动化生产。框架的部署方式有以下几种：
	- 命令行界面
		- 通过 命令行界面(Command Line Interface, CLI) 进行默认交互
	- 可视化界面
		- 通过 Gradio 提供基于 Web 的 GUI
	- API 服务器
		- 可以使用 FastAPI 包装代理并将其用作 API 端点
	
# 2、通用大模型
- OpenAI：**GPT-3/4**
- Google：LaMDA/PaLM
- Meta AI：**LLaMA2（开源）** 
- 百度：文心
- 阿里：千问
- 讯飞：星火
- 清华：**GLM3（开源）**


#### 模型的参数，以ChatGLM-6B为例
- 训练数据量：约 1T tokens（一般将一个Token视为一个单词，并为其分配一个数字，而ChatGPT不一定是一个单词一个Token，有可能单词的一部分视为一个Token，在ChatGPT中，平均大概1Token = 0.75个单词）
- 模型参数：6B（billion）
- 词表大小：130528（中文词表）
#### 开源还是闭源
- Hugging Face：Hugging Face是一个汇聚各类开源大模型的平台，它有三大法宝：预训练的开源模型、数据集和各种工具。
	- Hugging Face就给开发者准备好了现成的开源模型、数据集和各种工具包，让开发者能够轻松地访问和使用先进的大语言模型，快速地为具体应用创建AI解决方案。就像大部分程序员都会去GitHub上找代码一样，很多AI的开发者都会去Hugging Face上找模型、调模型。

# 3、行业大模型
- ChatLaw（法律）
- DoctorGLM（医疗）
- FinGPT（金融）
- MediaGPT（新媒体）
- TransGPT（交通）
- TechGPT（科研）
- 软件行业
	- Copilot
	- aiXcoder
	- iFlyCode
	- **Code Llama**：代码生成大语言模型
	- CodeGeeX
	- CodeShell
	- CodeArts Snap


# 4、大模型开发
- 数据
	- 数据源（代码库、文档）
		- 典型的训练数据集由来自多种来源的**文本数据**组成，例如**爬取的公共数据、在线出版物或书籍库、来自GitHub的代码数据、维基百科、新闻、社交媒体对话等**。例如，考虑**The Pile**。The Pile是由EleutherAI创建的用于大规模语言建模的流行文本语料库。它包含来自22个数据源的数据，粗略地分为五个大类：
			- 学术写作：PubMed摘要和PubMed Central，arXiv，FreeLaw，USPTO背景，PhilPapers，NIH Exporter
			- 在线或抓取的资源：CommonCrawl，OpenWebText2，Stack Exchange，Wikipedia
			- 散文：BookCorpus2，Bibliotik，Project Gutenberg
			- 对话：YouTube字幕，Ubuntu IRC，OpenSubtitles，Hacker News，Europarl
			- 杂项：GitHub，DeepMind数学数据集，Enron电子邮件
		- The Pile数据集是为公众免费提供的极少数大规模文本数据集之一。对于大多数现有的模型，如GPT-3、PaLM和Galactica，它们的训练和评估数据集并不公开。考虑到编译和预处理这些数据集以进行LLM训练所需的大规模努力，大多数公司都将它们保留在内部以保持竞争优势。这使得像The Pile和AllenAI的少量数据集对于公共大规模NLP研究目的非常有价值。
		- 在数据集收集期间，一般数据可以由非专家收集，但特定领域的数据通常需要由专业主题专家（SMEs）收集或咨询，例如医生、物理学家、律师等。
	- 数据预处理
		- 数据过滤与采样
		- 数据清洗
		- 数据去重（定义词汇表）
		- Tokenization（分词编码）
		- Embbeding（分词嵌入）
- 算法
	- 模型架构：
		- 自编码器（Autoencoders）
		- 自回归模型（Autoregressors）
		- 序列到序列模型（Sequence-to-Sequence）
	- 超参数设置：
		- 如学习率
		- 批量大小
		- 迭代次数
	- 微调
		- RAG
		- RLHF
	- 分布式训练框架
		- 如何选择
			- 训练成本：不同的训练工具，训练同样的大模型，成本是不一样的。对于大模型，训练一次动辄上百万/千万美元的费用。合适的成本始终是正确的选择。
			- 训练类型：是否支持数据并行、张量并行、流水线并行、多维混合并行、自动并行等
			- 效率：将普通模型训练代码变为分布式训练所需编写代码的行数，我们希望越少越好。
			- 灵活性：你选择的框架是否可以跨不同平台使用？
		- 常见的训练框架：
			- 第一类：深度学习框架自带的分布式训练功能。如：TensorFlow、PyTorch、MindSpore、Oneflow、PaddlePaddle等。
			- 第二类：基于现有的深度学习框架（如：PyTorch、Flax）进行扩展和优化，从而进行分布式训练。如：Megatron-LM（张量并行）、DeepSpeed（Zero-DP）、Colossal-AI（高维模型并行，如2D、2.5D、3D）、Alpa（自动并行）等
		- 两条实施路线：
			- TPU + XLA + TensorFlow/JAX ：由Google主导，由于TPU和自家云平台GCP深度绑定
			- GPU + PyTorch + Megatron-LM + DeepSpeed ：由NVIDIA、Meta、MicroSoft大厂加持，社区氛围活跃，也更受到大家欢迎。
- 算力
	- GPU（多卡）
	- 集群（多机）
	- 并行化
		- 数据并行（如：PyTorch DDP）
		- 模型/张量并行（如：Megatron-LM（1D）、Colossal-AI（2D、2.5D、3D））
		- 流水线并行（如：GPipe、PipeDream、PipeDream-2BW、PipeDream Flush（1F1B））
		- 多维混合并行（如：3D并行（数据并行、模型并行、流水线并行））
		- 自动并行（如：Alpa（自动算子内/算子间并行））
		- 优化器相关的并行（如：ZeRO（零冗余优化器，在执行的逻辑上是数据并行，但可以达到模型并行的显存优化效果）、PyTorch FSDP）
- 对于以Transformer、MOE结构为代表的大模型来说，传统的单机单卡训练模式肯定不能满足上千（万）亿级参数的模型训练，这时候我们就需要解决内存墙和通信墙等一系列问题，在**单机多卡或者多机多卡**进行模型训练。



# 9、实战
- 环境
	- 硬件环境
		- CPU
		- 显卡：GPU
			- 单卡
			- 多卡
		- 内存

	- 软件环境
		- Windows
		- Ubuntu
		- Centos
		- Mac



- 如何从零训练一个大语言模型？
- 如何部署一个已经训练好的大模型？
	- 案例：搭建环境实现本地部署ChatGLM2 6B 大模型
- 如何基于chatGLM-6B模型预训练，添加自己的数据集微调？
	- 案例：基于chatGLM-6B模型预训练，添加自己的数据集微调
- 如何开发一个大模型应用？
	- 案例：	基于大模型构建个人知识库助手
- 如何开发一个简单的智能体系统？
	- 案例：Meta GPT


# 10、大模型挑战
1. 数据安全隐患：一方面大模型训练需要大量的数据支持，但很多数据涉及到机密以及个人隐私问题，如客户信息、交易数据等。需要保证在训练大模型的同时保障数据安全，防止数据泄露和滥用。OpenAI在发布ChatGPT模型的时候用了数月来保证数据安全以及符合人类正常价值观标准。
2. 成本高昂：大模型的训练和部署需要大量的计算资源和人力资源，成本非常高昂。对于一些中小型企业而言，难以承担这些成本，也难以获得足够的技术支持和资源。
3. 无法保障内容可信：大模型会编造词句，无法保障内容真实可信、有据可查。当前使用者只能根据自己需求去验证生成的内容是否真实可信，很难具有权威说服力。
4. 无法实现成本可控：直接训练和部署千亿级参数大模型成本过高，**企业级应用应使用百亿级基础模型，根据不同需求训练不同的垂直模型，企业则只需要负担垂直训练成本。但是，如何实现高效的垂直训练，如何控制成本，仍是大模型面临的问题之一**。
5. 以上挑战依然有很大空间值得改进，需要进一步研究和探索新的技术和方法。比如可以采用数据加密、隐私保护等技术来保障数据安全；可以通过改进模型架构、优化训练算法、利用分布式计算等方式来提高大模型的效率和性能；此外，还可以通过开源和共享模型资源来降低成本、促进大模型的普及和应用等方式。