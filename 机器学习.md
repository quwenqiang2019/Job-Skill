# 机器学习模型

![](./pic/p12.jpg)



## 1、机器学习模型分类
- 单模型和集成模型
	- 单模型，是指机器学习模型仅包括一个模型，以某种模型独立进行训练和验证使用的。监督学习模型中大多数模型都可以算作单模型，包括线性回归、逻辑回归、Lasso回归、Ridge回归、线性判别分析、近邻、决策树、感知机、神经网络、支持向量机和朴素贝叶斯等。
	- 与单模型相对立的，就是集成模型，集成模型就是将多个单模型进行组合构成一个强模型，这个强模型能取所有单模型之所长，达到一个相对的最佳性能。集成模型中的单模型既可以是同种类别的，也可以是不同类别的，总体呈现一种“多而不同”的特征。常用的集成模型包括Boosting和Bagging两大类，主要包括AdaBoost、GBDT、XGBoost、LightGBM、CatBoost和随机森林等模型。
![](./pic/p13.png)

- 有监督模型和无监督模型
	- 监督模型是指模型在训练过程中根据数据输入和输出进行学习，监督学习模型包括分类（classification）、回归（regression）和标注（tagging）等模型。
	- 无监督模型是指从无标注的数据中学习得到模型，主要包括聚类（clustering）、降维（dimensionality reduction）和一些概率估计模型。
![](./pic/p14.png)

- 生成式模型和判别式模型（有监督模型的进一步划分）
	- 生成式模型的学习特点在于学习数据的联合概率分布，然后基于联合分布求条件概率分布作为预测模型。常用的生成式模型包括朴素贝叶斯、隐马尔可夫模型以及隐含狄利克雷分布模型等。
	- 判别式模型的学习特点在于基于数据直接学习决策函数或者条件概率分布作为预测模型，判别式模型关心的是对于给定的输入，应该预测出什么样的。常用的判别式模型有很多，像线性回归、逻辑回归、Lasso回归、Ridge回归、线性判别分析、近邻、决策树、感知机、神经网络、支持向量机、最大信息熵模型、全部集成模型以及条件随机场等，都属于判别式模型。
![](./pic/p15.png)

- 概率模型和非概率模型
	- 通过对输入和输出之间的联合概率分布和条件概率分布进行建模的机器学习模型，都可以称之为概率模型。而通过对决策函数建模的机器学习模型，即为非概率模型。
	- 常用的概率模型包括朴素贝叶斯、隐马尔可夫模型、贝叶斯网络和马尔可夫链蒙特卡洛等，而线性回归、近邻、支持向量机、神经网络以及集成模型都可以算是非概率模型。
	- 需要注意的是，概率模型与非概率模型的划分并不绝对，有时候有些机器学习模型既可以表示为概率模型，也可以表示为非概率模型。比如说决策树、逻辑回归、最大熵模型和条件随机场等模型，就兼具概率模型和非概率模型两种解释。概率模型和非概率模型的划分如图5所示。
![](./pic/p16.png)


## 2、机器学习三大分支
- 监督学习
- 无监督学习
- 强化学习

## 3、机器学习流程

对于表格数据，一套完整的机器学习建模流程如下：

- 1、准备数据
- 2、数据预处理：提取类别特征（object）：数值没有含义，和数值特征（非object）：数值大小有一定的含义
	- 数据基本信息
		- df.head()
		- df.info()
		- 数据量：df.shape
		- 字段名与类型：df.columns、df.dtypes
	- 错误数据处理
		- 类别型（字符串规范化的处理）
		- 数值型（类型、规范化的处理）
	- 特征编码：针对类别特征，将类别型转化为数值型，用数字代替类别的含义，数字的大小本身没有意义，只是一种类别的符号
		- 分类型（分类型变量：如性别，取值只有男、女，如指标的阳性和阴性）——独热编码（One-Hot Encoding/LabelEncoder）
		- 顺序型（顺序型变量：如成绩等级，取值分为优良中差）——顺序编码（Ordinal Encoding）
	- 数据清洗：（将编码后的类别特征当成是离散型数值特征）
		- 重复值处理
			- 删除法
		- 缺失值处理
			- 删除法
			- 填充法
				- 缺失值较少
					- 均值填充（连续性）
					- 众数填充（离散型）
				- 缺失值较多
					- 随机采样填充（连续型或离散型）
			- 预测法
		- 异常值处理（仅针对数值特征，类别特征数值代表类别，经过编码不存在异常）
			- 标准差检测（替换/删除）
			- 箱线图检测（替换/删除）
	- 数据探索
		- 特征分布（分离散型特征和连续型特征的分布情况）
		- 特征相关性
- 3、归一化和标准化（特定的模型需要）
	- 归一化：离散型
	- 标准化：连续型
- 4、特征重要性分析与筛选
- 5、提取特征变量和目标变量
- 6、数据集划分（交叉验证）
- 7、模型建立与训练（交叉验证）
- 8、模型的特征重要性分析
- 9、模型推理与评价
- 10、模型的优化和部署


## 4、分类预测模型
- 模型
	- 线性模型（linear_model）
		- 线性回归（from sklearn.linear_model import LinearRegression）
		- Lasso回归（from sklearn.linear_model import LassoCV）
	- 支持向量机（from sklearn.svm import SVR）
	- KNN（from sklearn.ensemble import BaggingRegressor）
	- **朴素贝叶斯（Naive Bayes）只能用于分类任务**
	- 树模型
		- 决策树（from sklearn.tree import DecisionTreeClassifier）
		- ExtraTree （from sklearn.tree import ExtraTreeRegressor）
	- 集成模型
		- 随机森林（from sklearn.ensemble import RandomForestRegressor）
		- 梯度提升树（from sklearn.ensemble import GradientBoostingRegressor）
		- AdaBoost（from sklearn.ensemble import AdaBoostRegressor）
		- Bagging（from sklearn.ensemble import BaggingRegressor）
		- Xgboost（from xgboost import XGBRegressor）
	- 多层感知机（from sklearn.neural_network import MLPRegressor）
- 比较评价指标
	- ROC曲线（from sklearn.metrics import roc_curve）
	- AUC（from sklearn.metrics import auc）
	- 分类报告（from sklearn.metrics import classification_report）
	- 混淆矩阵（from sklearn.metrics import confusion_matrix）
		- 准确率（from sklearn.metrics import accuracy_score）
		- 精确率（from sklearn.metrics import precision_score）
		- 召回率（from sklearn.metrics import recall_score）
		- F1值（from sklearn.metrics import f1_score）
	- NRI（净重新分类指数）
	- IDI（综合判别改善指数）
	- YI（约登指数）
	- KS曲线、Lift值、P-R曲线
	- kappa系数

## 5、回归预测模型
- 模型
	- 线性模型（linear_model）
		- 线性回归（from sklearn.linear_model import LinearRegression）
		- Lasso回归（from sklearn.linear_model import LassoCV）
		- Ridge回归（from sklearn.linear_model import Ridge）
	- 支持向量机（from sklearn.svm import SVR）
	- KNN（from sklearn.ensemble import BaggingRegressor）
	- **朴素贝叶斯（Naive Bayes）不能用于回归任务**
	- 树模型
		- 决策树（from sklearn.tree import DecisionTreeRegressor）
		- ExtraTree （from sklearn.tree import ExtraTreeRegressor）
	- 集成模型
		- 随机森林（from sklearn.ensemble import RandomForestRegressor）
		- 梯度提升树（from sklearn.ensemble import GradientBoostingRegressor）
		- AdaBoost（from sklearn.ensemble import AdaBoostRegressor）
		- Bagging（from sklearn.ensemble import BaggingRegressor）
		- Xgboost（from xgboost import XGBRegressor）
	- 多层感知机（from sklearn.neural_network import MLPRegressor）
- 评价指标
	- explained_variance_score(解释方差分)（from sklearn.metrics import explained_variance_score）
	- Mean absolute error（平均绝对误差）（from sklearn.metrics import mean_absolute_error）
	- Mean squared error（均方误差）（from sklearn.metrics import mean_squared_error）
	- Mean squared logarithmic error（from sklearn.metrics import mean_squared_log_error）
	- Median absolute error（中位数绝对误差）（from sklearn.metrics import median_absolute_error）
	- R² score（决定系数、R方）（from sklearn.metrics import r2_score）
	- Adjusted R-Square (校正决定系数）（1-((1-r2_score(y_test,y_predict))*(n-1))/(n-p-1)）
	- MAPE（平均绝对百分比误差（Mean Absolute Percentage Error））


## 6、机器学习或深度学习模型调优的方法
- 超参数调整：调整模型的超参数，例如学习率、正则化参数、批量大小等。可以使用网格搜索、随机搜索或贝叶斯优化等方法来搜索最佳的超参数组合。
- 模型架构调整：尝试不同的模型架构或层数，例如增加或减少隐藏层的数量、调整神经元的数量等。可以通过增加或减少模型的复杂性来改善性能。
- 特征工程：尝试不同的特征表示方法或特征组合方式。可以使用不同的特征选择、降维技术或特征交叉来提取更有信息量的特征。
- 数据增强：对训练数据进行增强，例如随机旋转、平移、缩放、翻转等操作，以扩充数据集并增加模型的泛化能力。
- 正则化：使用正则化技术来控制模型的复杂性，防止过拟合。常见的正则化方法包括L1正则化、L2正则化和Dropout。
- 集成方法：尝试使用集成学习方法，如随机森林、梯度提升树等，将多个模型的预测结果进行组合，以提高性能和稳定性。
- 迁移学习：利用预训练的模型或在相关任务上训练的模型来初始化或微调目标任务的模型。迁移学习可以加速模型的训练过程并提高性能。
- 早停策略：使用早停技术，在验证集上监测模型性能，并在性能不再提高时停止训练，以防止过拟合。
- 批量归一化：在深度神经网络中使用批量归一化技术，可以加速训练过程、提高模型的稳定性和泛化能力。
- 模型集成：将多个模型的预测结果进行组合，例如投票、加权平均等，以获得更好的性能。

## 7、模型的鲁棒性和泛化能力
- 鲁棒性：对模型而言，一些异常数据对模型的性能影响程度
- 泛化能力：模型对未知数据的预测能力



## 8、机器学习项目组织
- 项目文件夹内部应该如下所示。
	- input（包含机器学习项⽬的所有输⼊⽂件和数据）
		- train.csv
		- test.csv
	- src（保存与项⽬相关的所有 python 脚本）
		- create_folds.py
		- train.py
		- inference.py
		- models.py
		- config.py
		- model_dispatcher.py
	- models（保存所有训练过的模型）
		- model_rf.bin
		- model_et.bin
	- notebooks（存储在笔记本）
		- exploration.ipynb
		- check_data.ipynb
	- README.md
	- LICENSE
