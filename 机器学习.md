# 机器学习模型

![](./pic/p12.jpg)

#### 线性模型
- 数学原理：最小二乘法、正则化
- 常见的模型：
	- 线性回归
	- 逻辑回归
	- Lasso
	- Ridge
	- LDA
#### 决策树模型：
- 数学原理：信息熵
#### 神经网络模型：
- 数学原理：梯度下降、信息熵
### 支持向量机模型
### K近邻模型


## 机器学习模型分类
- 单模型和集成模型
	- 单模型，是指机器学习模型仅包括一个模型，以某种模型独立进行训练和验证使用的。监督学习模型中大多数模型都可以算作单模型，包括线性回归、逻辑回归、Lasso回归、Ridge回归、线性判别分析、近邻、决策树、感知机、神经网络、支持向量机和朴素贝叶斯等。
	- 与单模型相对立的，就是集成模型，集成模型就是将多个单模型进行组合构成一个强模型，这个强模型能取所有单模型之所长，达到一个相对的最佳性能。集成模型中的单模型既可以是同种类别的，也可以是不同类别的，总体呈现一种“多而不同”的特征。常用的集成模型包括Boosting和Bagging两大类，主要包括AdaBoost、GBDT、XGBoost、LightGBM、CatBoost和随机森林等模型。
![](./pic/p13.png)

- 有监督模型和无监督模型
	- 监督模型是指模型在训练过程中根据数据输入和输出进行学习，监督学习模型包括分类（classification）、回归（regression）和标注（tagging）等模型。
	- 无监督模型是指从无标注的数据中学习得到模型，主要包括聚类（clustering）、降维（dimensionality reduction）和一些概率估计模型。
![](./pic/p14.png)

- 生成式模型和判别式模型（有监督模型的进一步划分）
	- 生成式模型的学习特点在于学习数据的联合概率分布，然后基于联合分布求条件概率分布作为预测模型。常用的生成式模型包括朴素贝叶斯、隐马尔可夫模型以及隐含狄利克雷分布模型等。
	- 判别式模型的学习特点在于基于数据直接学习决策函数或者条件概率分布作为预测模型，判别式模型关心的是对于给定的输入，应该预测出什么样的。常用的判别式模型有很多，像线性回归、逻辑回归、Lasso回归、Ridge回归、线性判别分析、近邻、决策树、感知机、神经网络、支持向量机、最大信息熵模型、全部集成模型以及条件随机场等，都属于判别式模型。
![](./pic/p15.png)

- 概率模型和非概率模型
	- 通过对输入和输出之间的联合概率分布和条件概率分布进行建模的机器学习模型，都可以称之为概率模型。而通过对决策函数建模的机器学习模型，即为非概率模型。
	- 常用的概率模型包括朴素贝叶斯、隐马尔可夫模型、贝叶斯网络和马尔可夫链蒙特卡洛等，而线性回归、近邻、支持向量机、神经网络以及集成模型都可以算是非概率模型。
	- 需要注意的是，概率模型与非概率模型的划分并不绝对，有时候有些机器学习模型既可以表示为概率模型，也可以表示为非概率模型。比如说决策树、逻辑回归、最大熵模型和条件随机场等模型，就兼具概率模型和非概率模型两种解释。概率模型和非概率模型的划分如图5所示。
![](./pic/p16.png)


## 机器学习三大分支
- 监督学习
- 无监督学习
- 强化学习

## 机器学习流程

数据预处理->模型选择与构建->模型的训练和优化->模型的部署




## 分类预测模型的比较评价指标
- AUC（ROC面积）
	- 准确率
	- 精确率
	- 召回率
- NRI（净重新分类指数）
- IDI（综合判别改善指数）
- YI（约登指数）


## 机器学习或深度学习模型调优的方法
- 超参数调整：调整模型的超参数，例如学习率、正则化参数、批量大小等。可以使用网格搜索、随机搜索或贝叶斯优化等方法来搜索最佳的超参数组合。
- 模型架构调整：尝试不同的模型架构或层数，例如增加或减少隐藏层的数量、调整神经元的数量等。可以通过增加或减少模型的复杂性来改善性能。
- 特征工程：尝试不同的特征表示方法或特征组合方式。可以使用不同的特征选择、降维技术或特征交叉来提取更有信息量的特征。
- 数据增强：对训练数据进行增强，例如随机旋转、平移、缩放、翻转等操作，以扩充数据集并增加模型的泛化能力。
- 正则化：使用正则化技术来控制模型的复杂性，防止过拟合。常见的正则化方法包括L1正则化、L2正则化和Dropout。
- 集成方法：尝试使用集成学习方法，如随机森林、梯度提升树等，将多个模型的预测结果进行组合，以提高性能和稳定性。
- 迁移学习：利用预训练的模型或在相关任务上训练的模型来初始化或微调目标任务的模型。迁移学习可以加速模型的训练过程并提高性能。
- 早停策略：使用早停技术，在验证集上监测模型性能，并在性能不再提高时停止训练，以防止过拟合。
- 批量归一化：在深度神经网络中使用批量归一化技术，可以加速训练过程、提高模型的稳定性和泛化能力。
- 模型集成：将多个模型的预测结果进行组合，例如投票、加权平均等，以获得更好的性能。

# 模型的鲁棒性和泛化能力
- 鲁棒性：对模型而言，一些异常数据对模型的性能影响程度
- 泛化能力：模型对未知数据的预测能力