# 1、目标和任务
- 开发出适用于软件工厂场景的大模型
- 基于大模型，开发多智能体软件开发框架（类似MetaGPT和ChatDev），通过这个框架能够实现软件的自动化生产。框架的部署方式有以下几种：
	- 命令行界面
		- 通过 命令行界面(Command Line Interface, CLI) 进行默认交互
	- 可视化界面
		- 通过 Gradio 提供基于 Web 的 GUI
	- API 服务器
		- 可以使用 FastAPI 包装代理并将其用作 API 端点
	
# 2、通用大模型
- OpenAI：**GPT-3/4**
- Google：LaMDA/PaLM
- Meta AI：**LLaMA2（开源）** 
- 百度：文心
- 阿里：千问
- 讯飞：星火
- 清华：**GLM3（开源）**
#### 模型的参数，以ChatGLM-6B为例
- 训练数据量：约 1T tokens
- 模型参数：6B（billion）
- 词表大小：130528（中文词表）
#### 开源还是闭源
- Hugging Face：Hugging Face是一个汇聚各类开源大模型的平台，它有三大法宝：预训练的开源模型、数据集和各种工具。
	- Hugging Face就给开发者准备好了现成的开源模型、数据集和各种工具包，让开发者能够轻松地访问和使用先进的大语言模型，快速地为具体应用创建AI解决方案。就像大部分程序员都会去GitHub上找代码一样，很多AI的开发者都会去Hugging Face上找模型、调模型。

# 3、行业大模型
- ChatLaw（法律）
- DoctorGLM（医疗）
- FinGPT（金融）
- MediaGPT（新媒体）
- TransGPT（交通）
- TechGPT（科研）
- 软件行业
	- Copilot
	- aiXcoder
	- iFlyCode
	- **Code Llama**：代码生成大语言模型



	- CodeGeeX
	- CodeShell
	- CodeArts Snap


# 4、大模型开发
- 数据
	- 数据源（代码库、文档）
		- 典型的训练数据集由来自多种来源的**文本数据**组成，例如**爬取的公共数据、在线出版物或书籍库、来自GitHub的代码数据、维基百科、新闻、社交媒体对话等**。例如，考虑**The Pile**。The Pile是由EleutherAI创建的用于大规模语言建模的流行文本语料库。它包含来自22个数据源的数据，粗略地分为五个大类：
			- 学术写作：PubMed摘要和PubMed Central，arXiv，FreeLaw，USPTO背景，PhilPapers，NIH Exporter
			- 在线或抓取的资源：CommonCrawl，OpenWebText2，Stack Exchange，Wikipedia
			- 散文：BookCorpus2，Bibliotik，Project Gutenberg
			- 对话：YouTube字幕，Ubuntu IRC，OpenSubtitles，Hacker News，Europarl
			- 杂项：GitHub，DeepMind数学数据集，Enron电子邮件
		- The Pile数据集是为公众免费提供的极少数大规模文本数据集之一。对于大多数现有的模型，如GPT-3、PaLM和Galactica，它们的训练和评估数据集并不公开。考虑到编译和预处理这些数据集以进行LLM训练所需的大规模努力，大多数公司都将它们保留在内部以保持竞争优势。这使得像The Pile和AllenAI的少量数据集对于公共大规模NLP研究目的非常有价值。
		- 在数据集收集期间，一般数据可以由非专家收集，但特定领域的数据通常需要由专业主题专家（SMEs）收集或咨询，例如医生、物理学家、律师等。
	- 数据预处理
		- 数据过滤与采样
		- 数据清洗
		- 数据去重
		- Tokenization
		- Embbeding
- 算法
	- 模型架构：
		- 自编码器（Autoencoders）
		- 自回归模型（Autoregressors）
		- 序列到序列模型（Sequence-to-Sequence）
	- 超参数设置：
		- 如学习率
		- 批量大小
		- 迭代次数
	- 微调
		- RAG
		- RLHF
	- 分布式训练框架
		- 如何选择
			- 训练成本：不同的训练工具，训练同样的大模型，成本是不一样的。对于大模型，训练一次动辄上百万/千万美元的费用。合适的成本始终是正确的选择。
			- 训练类型：是否支持数据并行、张量并行、流水线并行、多维混合并行、自动并行等
			- 效率：将普通模型训练代码变为分布式训练所需编写代码的行数，我们希望越少越好。
			- 灵活性：你选择的框架是否可以跨不同平台使用？
		- 常见的训练框架：
			- 第一类：深度学习框架自带的分布式训练功能。如：TensorFlow、PyTorch、MindSpore、Oneflow、PaddlePaddle等。
			- 第二类：基于现有的深度学习框架（如：PyTorch、Flax）进行扩展和优化，从而进行分布式训练。如：Megatron-LM（张量并行）、DeepSpeed（Zero-DP）、Colossal-AI（高维模型并行，如2D、2.5D、3D）、Alpa（自动并行）等
		- 两条实施路线：
			- TPU + XLA + TensorFlow/JAX ：由Google主导，由于TPU和自家云平台GCP深度绑定
			- GPU + PyTorch + Megatron-LM + DeepSpeed ：由NVIDIA、Meta、MicroSoft大厂加持，社区氛围活跃，也更受到大家欢迎。
- 算力
	- GPU（多卡）
	- 集群（多机）
	- 并行化
		- 数据并行（如：PyTorch DDP）
		- 模型/张量并行（如：Megatron-LM（1D）、Colossal-AI（2D、2.5D、3D））
		- 流水线并行（如：GPipe、PipeDream、PipeDream-2BW、PipeDream Flush（1F1B））
		- 多维混合并行（如：3D并行（数据并行、模型并行、流水线并行））
		- 自动并行（如：Alpa（自动算子内/算子间并行））
		- 优化器相关的并行（如：ZeRO（零冗余优化器，在执行的逻辑上是数据并行，但可以达到模型并行的显存优化效果）、PyTorch FSDP）
- 对于以Transformer、MOE结构为代表的大模型来说，传统的单机单卡训练模式肯定不能满足上千（万）亿级参数的模型训练，这时候我们就需要解决内存墙和通信墙等一系列问题，在**单机多卡或者多机多卡**进行模型训练。

# 5、智能体
- Agent
	- **智能体 = LLM + 记忆（Memory：存储历史信息） + 规划（Planning：生成计划决策） + 工具（Action：执行计划决策） + 执行流程/思维链（思考）**,各种不同的 AI Agent的差异与区别也几乎都从上述四部分展开
	![](pic/36.png)
- Agent分类
	- **自主智能体（Autonomous Agent），单智能体**，其核心思想是“像人类智能一样去解决问题”，类似 AutoGPT 的智能体。
	- **生成智能体（Generative Agent），多智能体**，生成智能体的核心在于“像社会智能一样去解决问题”，以斯坦福小镇为代表。
- Agent与LLM的区别联系
	- Agent 与一般的 LLM 最大的不同点在于，LLM Agent 通常根据任务的总体目标来去指定以及编排子目标，而 LLM 通常是作为一个被调用的工具，在一个工作流中担任一个具体任务的执行者。
	- 目前和 LLMs 的交互基本都是这样的：人类输入 Prompt→LLMs 计算响应。每次如果想要新的响应都需要重新输入 Prompt，而这一步骤必须依靠人类主动触发。AI agents 则可以理解成一种不同的工作方式，它们可以自我提问、自行解决问题、举一反三以及自主行动，之后还可以自行确定行动的结果，作为人类，我们只需要一开始给定一个目标即可——比如说告诉它研究特定内容。面对一个“任务”，由人类站在高点描述一个“任务目标”，并将完成这一任务的工作交予 Agent，而 AI 接受目标并自主的进行“感知环境”，“形成记忆”，“完成规划”，“决策行动”，“观察纠错”等一系列以任务目标为导向的行动。
	-  下一代 AI 技术走向并非是生成性 AI，而应该是**交互性 AI**，Agent技术是未来实现社会全面自动化的关键技术
- 如何实现Agent
	- 基于 Prompt （COT、ReAct、Reflexion等一系列方法）
	- 基于构建LLM的智能体的框架与工具包（具备较强的二次开发能力）
		- 基于 LangChain 
		- 基于Camel
	- 基于 Agents 多智能体框架
	- 基于开箱即用的专注于某些特定任务的智能体（二次开发能力相对较弱）
		- AutoGPT
		- MetaGPT

# 6、几个相关的Agent项目
## 6.1 agents
- 项目地址：https://github.com/aiwaves-cn/agents
## 6.2 AutoGPT/AgentGPT
- 项目地址：https://github.com/Significant-Gravitas/AutoGPT
- 项目地址：https://github.com/reworkd/AgentGPT
## 6.3 open-interpreter
- 项目地址：https://github.com/KillianLucas/open-interpreter
## 6.4 AutoGen
- 项目地址：https://github.com/microsoft/autogen
- 是一个 LLMs **框架**，类似于 LangChain，主要用于简化 LLMs 工作流的编排、优化和自动化。它提供了可定制和可对话的 agents，并且对于人工参与度的支持非常好，可以在使用 LLMs、人工输入和工具组合的各种模式下运行。
## 6.5 AgentVerse
- 项目地址：github.com/OpenBMB/AgentVerse
- 论文地址：arxiv.org/abs/2308.10848
## 6.6 Meta GPT
- 项目地址：https://github.com/geekan/MetaGPT
- 论文地址：https://arxiv.org/pdf/2308.00352.pdf
- 案例演示：
	- 环境
		- win10（cmd）
		- Anaconda(python3.10)
		- git工具
		- 科学上网工具
		- OPENAI的api_key(本文采用GPT4模型)
		- npm工具
	- 步骤
		- 第一步：克隆仓库到某个文件夹下，如在D:\workspace\software-factory下执行
			- git clone https://github.com/geekan/metagpt
		- 第二步：创建anaconda环境并激活
			- conda create -n metagpt python=3.10
			- conda activate metagpt
		- 第四步：到项目文件夹下，安装依赖包
			- npm install -g @mermaid-js/mermaid-cli
			- python setup.py install
			- pip install -r requirements.txt
		- 第五步：**配置OPENAI的密钥，在 MetaGPT/config 目录中，创建 key.yaml文件，填入自己的OpenAI的API key和OPENAI_API_BASE 参数**，或者直接在config 目录中的config.yaml文件进行配置，或者在cmd窗口输入以下命令：
			- set OPENAI_API_KEY=your_api_key
			- set OPENAI_API_BASE = your_api_base
		- 第六步：构建软件
			- python startup.py "Write a cli snake game"
			- 定制化需求
				- 在win10操作系统环境下，基于python3.10解释器，爬取豆瓣电影Top250的相关信息，包括电影详情链接,图片链接,影片中文名,影片外国名,评分,评价数,概况,导演,主演,年份,地区,类别这12项内容，并将爬取的信息写入Excel表中
- 项目文件：
	- startup.py：代码入口，可以设置想花多少钱，需要几个角色合作……
	- actions：使用 PROMPT 方式，通过调用 GPT 和其它 API 实现了具体的功能，比如：写代码、分析库依赖、文本转语音、运行代码、测试……
	- document_store：各种存储的支持，faiss, milvus, chromadb向量数据库，本地存储，各种模式数据的读取，
	- management：管理所有技能
	- memory：管理存储
	- prompts：管理各种角色的提示
	- provider：调用大语言模型，目前支持Claude和OpenAI
	- roles：各角色的具体实现和相关工具，角色包含：工程师，架构师，产品，项目管理……
	- tools & utils：辅助工具

		![](pic/33.jpg)

- 实现原理：角色定义、SOP程序化、信息共享机制
	- 在系统中定义了几种角色，并为每种角色配备了目标和prompt模板，以此引导相关角色解决相应问题，这些角色都有自己的目标以及输入和输出，为每种角色定义了一个单独的“**进程**”运行，每个角色会**监听**有关自己的任务输入。每一个角色在运行中都是**等待**是否有相应的输入出现，一旦观察到相应的输入就会立马根据自己的目标，使用大模型来解决这个问题（作出行动），并将结果返回到系统中。
		- 产品经理
			- 目标：创造一个成功的产品
			- 观察的输入：观察老板是否有新的需求
			- 可以的行动：写产品需求文档
		- 架构师
			- 目标：根据产品需求文档设计系统架构
			- 观察到输入：观察产品经理是否有新需求
			- 可以的行动：写设计文档
		- 项目经理
			- 目标：提升团队的效率
			- 观察的输入：观察是否有新的设计任务
			- 可以的行动：根据设计文档完成任务分解
		- 工程师
			- 目标：根据项目经理分解的任务完成代码和评审
			- 观察的输入：观察项目经理分解的任务
			- 可以的行动：写代码/代码评审
		- QA工程师
			- 目标：根据代码的输出和测试代码输出以及运行结果输出测试方案
			- 观察的输入：代码的运行结果
			- 可以的行动：写测试用例
	- 这些角色**能力和技能都是一个单独的文件**，**每一个py文件里面定义的都是对应技能的prompt模板**，每一个角色都可以通过引入这种技能来增强自己的能力，最终各个角色都是通过自己所有的这些技能来处理输入和输出，完成任务：
		- 分析代码库：analyze_dep_libs.py
		- azure的语音合成：azure_tts.py
		- debug：debug_error.py
		- 设计api：design_api.py
		- api评审：design_api_review.py
		- 设计文件名：design_filenames.py
		- 项目管理：project_management.py
		- 代码运行：run_code.py
		- 搜索和摘要：search_and_summarize.py
		- 写代码：write_code.py
		- 写代码评审：write_code_review.py
		- 写需求说明书：write_prd.py
		- 写需求说明书评审：write_prd_review.py
		- 写测试用例：write_test.py

		![](pic/29.jpeg)

	- 整个过程通过python**协程异步并发**的方式运行
		- 入口是startup.py
			- 开一个公司（from metagpt.software_company import SoftwareCompany）
			- 雇佣不同的role（from metagpt.roles import (Architect,Engineer,ProductManager,ProjectManager,QaEngineer,)）
			- **会调用SoftwareCompany类的run方法（公司开始运行）**
		- software_company.py
			- **调用start_project方法**，以BOSS role通过publish_message方法输入一个需求message，这个message被丢到environment的memory中
			- **会调用environment对象的run方法（环境搭建，真正的入口）**
		- environment.py（环境，承载一批角色，角色可以向环境发布消息，可以被其他角色观察到）
			- **会调用各个role对象的run方法**（开始干活）
			- role的方法逻辑，role之间的交互
				- role通过publish_message方法向environment发message，message存在environment的memory中
				- 观察environment中的信息（_observe方法）
					- 从environment的memory中读取消息，然后在通过get_by_actions方法去获得由role的_watch方法指定的要观察的对象其某个action产生的message，作为self._rc.news，然后role将这些news存到role的memory中
				- 如果需要处理，也就是role通过_observe方法观察到了news，则进行处理并将结果发送回environment中（_react方法）
					- 调用_think方法：设置state
					- 调用_act方法：会从role（自己）的memory中获取需要的message，作为自己action的输入（作为prompt交给GPT处理），将结果存到自己role的memory中和environment的memory中
	
			![](pic/30.jpeg)

- 思考总结
	- MetaGPT是怎么抽象出的软件公司开发流程的？SOP具体在代码上是怎么实现的？
		- SOP就是不同role之间交互的顺序，比如产品经理需要BOSS的message作为他prompt的约束，而产品经理的产出是架构师的输入，不同角色间的输入输出，就是SOP。
	- MetaGPT是怎么抽象具体的职业的？比如产品经理是怎么抽象的?
		- 从职业这个角度讲，主要通过action和承接的message来抽象，比如产品经理，就抽象成，接收老板需求，产出产品需求文档，将需求文档给到架构师的对象，然后每个role生成的结果都会放到env中，其他人也可以看到（很多角色只有一个action，就不会用到env中的message）
	- MetaGPT中不同AI Agent是怎么交互的？
		- role通过_watch确定要从哪个role的哪个action中获得这个action的输出，具体获取的过程在_observer方法中，获得其他role的message后，如果当前的role有多个action可执行，则通过_think去选一个action（使用env的memory，即当前环境中发生的内容），再通过_act去具体的执行action，执行action时，会从role的memory中获取需要的message。
	- 从效果上看，MetaGPT输出的内容是比较格式化的，要做到这样的效果，prompt是怎么写的？
		- MetaGPT的prompt的设计形式值得学习，它主要使用MarkDown格式来设计prompt，多数prompt中都会有context、example，从而让GPT4更好的发挥zero-shot能力，想知道具体的，建议直接拉代码下来看。
	- MetaGPT 目前已经达到一个实用状态，已经能完成**小型**的软件需求，网友们使用MetaGPT完成了包括贪吃蛇、打砖块游戏、2048网页版、Flappy Bird及学生管理系统等软件的开发，十分实用。

- 存在的缺陷
	- 运行程序时，依赖的库需要手动安装
		- ModuleNotFoundError: No module named 'openpyxl'
	- 容易出现库的版本兼容问题，在写需求时应该注明python或其他语言的版本
		- ImportError: cannot import name 'url_quote' from 'werkzeug.urls' (D:\software\Anaconda3\envs\metagpt\lib\site-packages\werkzeug\urls.py)
		- 编码问题
	- 代码审查并不能检查出所以问题，比如导包
	- 程序最终也无法运行
	- 对此我对需求进行完善，并用GPT4模型进行实验



## 6.7 ChatDev
- 环境：
	- win11
	- Anaconda(python3.10)
	- git
	- 科学上网工具
	- OPENAI的api_key(本文采用GPT4模型)
- 步骤
	- 克隆GitHub存储库： 首先，使用在cmd命令工具下用以下命令克隆存储库：
		- git clone https://github.com/OpenBMB/ChatDev.git
		- 在D:\workspace\software-factory就会出现项目文件夹D:\workspace\software-factory\ChatDev
	- 设置Python环境： 使用以下命令创建anaconda环境chatdev，并激活环境：
		- conda create -n chatdev python=3.10
		- conda activate chatdev
	
		![](pic/ChatDev/1.png)
	
	- 安装依赖项： 进入ChatDev目录并运行以下命令来安装必要的依赖项：
		- pip install -r requirements.txt
	
		![](pic/ChatDev/2.png)
	
	- 设置OpenAI API密钥：在Windows系统cmd上：
		- set OPENAI_API_KEY=your_api_key
		- set OPENAI_API_BASE = your_api_base
	
		![](pic/ChatDev/3.png)
	
	- 构建软件： 使用以下命令启动生成您的软件，将[design a basic Gomoku game]替换为您的想法描述，将[Gomoku] 替换为您想要的项目名称：
		- python run.py --task "[design a basic Gomoku game]" --name "[Gomoku]"
	
		![](pic/ChatDev/4.png)
	
	- 运行软件： 生成后，在WareHouse 目录下的特定项目文件夹中找到您的软件，例如[Gomoku]_DefaultOrganization_20231108135427。在该目录中运行以下命令来运行软件：
		- cd WareHouse/[Gomoku]_DefaultOrganization_20231108135427
		- python main.py

		![](pic/ChatDev/5.png)

- 结果
	- 运行效果
	
      		 ![](pic/ChatDev/6.jpg)

	- 项目文件夹
	
		![](pic/ChatDev/7.jpg)

- 原理解析
	- 角色设定：CEO、CTO、CPO、Designer、Programmer、Reviewer、Tester
	- 开发流程设定：整个过程划分为设计、编码、测试和文档四个阶段
	- 工作驱动：聊天链（ChatChain）架构
	- 工作流程：
		- 设计阶段：
			- 由 CEO、CPO、CTO 三方会谈，
			- 从产品顶层出发，确定软件的主要功能形式、整体架构，以及开发语言
		- 编码阶段：
			- 由 CTO、Programmer和Designer主导本阶段工作。
			- 聊天链将此过程分解为多个子任务，通常由两个角色参与完成。例如， CTO 提出具体功能规格说明，程序员就生成 Python 代码。美工设计 GUI 界面，程序员将其与代码集成。
			- 关键技术
				- 使用面向对象的编程语言，其模块化和继承等特性可减少整个系统的代码冗余度。
				- 使用代码演化（Version Evolution）机制，通过 Git 版本控制，仅将最新版的源代码呈现给交流链的各个环节
				- 为了缓解代码幻觉问题，本文提出一种思维指示（Thought Instruction）的策略以缓解潜在的代码幻觉。
		- 测试阶段：
			- 由Programmer、Reviewer、Tester参与完成。
			- 先是代码评审（code review），其过程与人类活动相似，由同行查看代码以查漏补缺。然后是测试人员使用解释器验证软件功能，以黑盒测试的方式完成系统测试。
		- 文档阶段：
			- 由 CEO、CPO、CTO、Programmer来写
			- 文档包括运行环境依赖文件、用户手册等。
	- 驱动智能体交流对话的主要机制：
		- 角色专业化
		- 记忆流
		- 自我反思
- 特点
	- 支持自定义ChatChain、Phase和Role设置
	- 支持在线Log模式和重放模式
	- 适合研究群体agent智能任务
	- 简单，易于上手

# 8、LangChain
- LangChain 是一个基于大语言模型的编程框架，主要目标是建立LLM与各种外部数据源之间的连接，可以帮助我们创建基于大语言模型的端到端应用程序或流程。

# 9、实战
- 如何从零训练一个大语言模型？
- 如何基于chatGLM-6B模型预训练，添加自己的数据集微调？
- 如何部署一个已经训练好的AI模型？
- 如何开发一个简单的智能体系统？


# 10、大模型挑战
1. 数据安全隐患：一方面大模型训练需要大量的数据支持，但很多数据涉及到机密以及个人隐私问题，如客户信息、交易数据等。需要保证在训练大模型的同时保障数据安全，防止数据泄露和滥用。OpenAI在发布ChatGPT模型的时候用了数月来保证数据安全以及符合人类正常价值观标准。
2. 成本高昂：大模型的训练和部署需要大量的计算资源和人力资源，成本非常高昂。对于一些中小型企业而言，难以承担这些成本，也难以获得足够的技术支持和资源。
3. 无法保障内容可信：大模型会编造词句，无法保障内容真实可信、有据可查。当前使用者只能根据自己需求去验证生成的内容是否真实可信，很难具有权威说服力。
4. 无法实现成本可控：直接训练和部署千亿级参数大模型成本过高，**企业级应用应使用百亿级基础模型，根据不同需求训练不同的垂直模型，企业则只需要负担垂直训练成本。但是，如何实现高效的垂直训练，如何控制成本，仍是大模型面临的问题之一**。
5. 以上挑战依然有很大空间值得改进，需要进一步研究和探索新的技术和方法。比如可以采用数据加密、隐私保护等技术来保障数据安全；可以通过改进模型架构、优化训练算法、利用分布式计算等方式来提高大模型的效率和性能；此外，还可以通过开源和共享模型资源来降低成本、促进大模型的普及和应用等方式。