## 数据探索
- 分桶（连续型数据离散化）
- 转化（非正态分布转正态分布）

## 数据清洗（对象是数值型特征）
- 异常值处理
	- 标准差检测（替换/删除）
	- 箱线图检测（替换/删除）
- 缺失值处理
	- 删除法
	- 填充法
		- 缺失值较少
			- 均值填充（连续性）
			- 众数填充（离散型）
		- 缺失值较多
			- 随机采样填充（连续型或离散型）
	- 预测法

## 特征重要性分析（特征筛选）
- 为什么特征重要性分析很重要?
	- 如果有一个包含数十个甚至数百个特征的数据集，每个特征都可能对你的机器学习模型的性能有所贡献。但是并不是所有的特征都是一样的。有些可能是冗余的或不相关的，这会增加建模的复杂性并可能导致过拟合。
	- 特征重要性分析可以识别并关注最具信息量的特征，从而带来以下几个优势:
		- 改进的模型性能
		- 减少过度拟合
		- 更快的训练和推理
		- 增强的可解释性

- 特征重要性分析方法
	- 排列重要性 PermutationImportance
		- 该方法会随机排列每个特征的值，然后监控模型性能下降的程度。如果获得了更大的下降意味着特征更重要
	- 内置特征重要性(coef_或feature_importances_)
		- 一些模型，如线性回归和随机森林，可以直接输出特征重要性分数。这些显示了每个特征对最终预测的贡献。
	- Leave-one-out
		- 迭代地每次删除一个特征并评估准确性。
	- 相关性分析
		- 计算各特征与目标变量之间的相关性。相关性越高的特征越重要。
	- 递归特征消除 Recursive Feature Elimination
		- 递归地删除特征并查看它如何影响模型性能。删除时会导致更大下降的特征更重要。
	- XGBoost特性重要性
		- 计算一个特性用于跨所有树拆分数据的次数。更多的分裂意味着更重要。
	- 主成分分析 PCA/PCR
		- 对特征进行主成分分析，并查看每个主成分的解释方差比。在前几个组件上具有较高负载的特性更为重要。
	- PLS-DA：偏最小二乘判别分析
	- 方差分析 ANOVA
		- 使用f_classif()获得每个特征的方差分析f值。f值越高，表明特征与目标的相关性越强。
	- 卡方检验
		- 使用chi2()获得每个特征的卡方统计信息。得分越高的特征越有可能独立于目标。

- 为什么不同的方法会检测到不同的特征?
	- 他们用不同的方式衡量重要性:
		- 有的使用不同特特征进行预测，监控精度下降
		- 像XGBOOST或者回国模型使用内置重要性来进行特征的重要性排列
		- 而PCA着眼于方差解释
	- 不同模型有不同模型的方法：
		- 线性模型倾向于线性关系、树模型倾向于接近根的特征
	- 交互作用:
		- 有的方法可以获取特征之间的相互左右，而有一些则不行，这就会导致结果的差异
	- 不稳定:
		- 使用不同的数据子集，重要性值可能在同一方法的不同运行中有所不同，这是因为数据差异决定的
	- Hyperparameters:
		- 通过调整超参数，如PCA组件或树深度，也会影响结果
		- 所以不同的假设、偏差、数据处理和方法的可变性意味着它们并不总是在最重要的特征上保持一致。
		
- 选择特征重要性分析方法的一些最佳实践
	- 尝试多种方法以获得更健壮的视图
	- 聚合结果的集成方法
	- 更多地关注相对顺序，而不是绝对值
	- 差异并不一定意味着有问题，检查差异的原因会对数据和模型有更深入的了解


## 特征编码方式
- 类别型：object
	- 分类型（分类型变量：如性别，取值只有男、女）——独热编码（One-Hot Encoding/LabelEncoder）
	- 顺序型（顺序型变量：如成绩等级，取值分为优良中差）——顺序编码（Ordinal Encoding）
- 数值型
	- 离散型：int（离散型数值变量：如车流量）——min-max标准化（Min-Max Normalization）
	- 连续型：fioat（连续型数值变量：如气温）——Z-score标准化
- 哪些模型特征需要编码
	- 关心变量值、基于距离读量的模型，使用梯度下降的算法，需要归一化：SVM, 逻辑回归，神经网络，KNN, 线性回归，Adaboost、KMeans、LSTM
	- 树模型是不能进行梯度下降的，因为树模型是阶越的，不可导。树模型是通过寻找特征的最优分裂点来完成优化的。由于归一化不会改变分裂点的位置树形结构的不需要归一化，如xgboost、lightGBM、GBDT
	- 概率模型不关心变量值，而关心变量的分布、变量之间的条件概率不需要归一化。这类模型像决策树、随机森林、朴素贝叶斯

- 标准化处理主要针对特征而言，通过将特征缩放到相同的尺度范围，可以避免某些特征对模型训练的影响过大。这样可以提高模型的稳定性和收敛速度，并且更容易找到合适的模型参数。
- 对于标签而言，通常不需要进行标准化处理。因为标签是我们要预测的目标值，我们希望模型能够准确地预测出实际的标签值，而不是标签的相对大小或分布。标签的原始值通常包含了我们要预测的信息，因此在回归问题中，一般不对标签进行标准化处理。
- 如果标签没有进行标准化处理，那么在预测阶段也不需要对预测结果进行反向标准化处理。可以直接使用模型预测得到的结果，这些结果将对应于原始数据的范围。如果标签也进行了标准化处理，需要使用训练数据集中标签的均值和标准差进行反向的标准化处理将预测结果转换回原始数据的范围。


## 归一化与反归一化
###在有些机器学习算法中，需要对特征进行标准化处理以消除量纲的差异，这样训练得到的模型，对于新的数据该怎么办呢？比如年龄经过标准化处理，但是对于一个新的数据，年龄是没有标准化处理的


- 在训练过程中，我们计算训练集的均值和标准差（或最大值和最小值），然后对训练集进行标准化处理。在进行预测时，对新的数据也要使用相同的均值和标准差（或最大值和最小值）进行标准化处理。
- 以年龄为例，如果在训练数据中对年龄进行了标准化处理，那么在使用训练得到的模型进行预测时，需要对新的数据中的年龄进行相同的标准化处理。具体步骤如下：
	- 计算训练数据集中年龄的均值和标准差（或最大值和最小值）。
	- 对训练数据集中的年龄进行标准化处理，得到标准化后的年龄数据。
	- 使用标准化后的数据训练模型。
	- **对于新的数据，使用与训练数据年龄相同的均值和标准差（或最大值和最小值）对年龄进行标准化处理。**
	- **使用训练得到的模型对标准化后的新数据进行预测。**
	- **如果需要得到原始数据的预测结果，可以将预测结果反向进行标准化处理。（如果标签没有标准化处理，则不需要这一步）**
- 总结来说，对于新的数据，需要按照与训练数据相同的方式进行标准化处理，以保持数据的一致性。这样可以确保模型在新数据上的预测结果具有可比性和准确性。